\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\title{Boosted Regression Trees for Model-Free Auto-Tuning}

\numberofauthors{3}

\author{
%\alignauthor First Last\titlenote{}\\
\alignauthor First Last\\
\affaddr{Affiliation line 1}\\
\affaddr{Affiliation line 2}\\
\email{anon@mail.com}
% 2nd author
%\alignauthor First Last\titlenote{None}\\
\alignauthor First Last\\
\affaddr{Affiliation line 1}\\
\affaddr{Affiliation line 2}\\
\email{anon@mail.com}
% 3rd author
%\alignauthor First Last\titlenote{None}\\
\alignauthor First Last\\
\affaddr{Affiliation line 1}\\
\affaddr{Affiliation line 2}\\
\email{anon@mail.com}
}

\begin{document}
\maketitle

\begin{abstract}

Autotuning is a widely used and effective technique for optimizing a
parametrized GPU code template for a particular type of computation on
particular hardware.
Its drawback is that exhaustive or even thorough autotuning can be very slow.
Furthermore, library abstraction boundaries provide operations such as image
filtering and matrix multiplication, which actually correspond to a large set
of potential problem configurations, with a potentially wide variety of memory
access patterns and computational bottlenecks.
How can we draw on data from previous auto-tuning of related problems on related hardware to make a
good decision on a novel problem?
This paper presents a machine learning approach to auto-tuning, in
which features of
(a) the current hardware platform,
(b) the kernel configuration
and (c) the problem instance are used to fit a regression model (boosted regression trees) that predicts
how much faster an implementation will be than a reference baseline.
Combinatorial optimization strategies that would normally be applied for autotuning can be applied to the regression model to find an implementation much more quickly.
We validate our approach using the filterbank correlation kernel described in [XXX].
We find that 0.05 seconds of a hill climbing on the regression model can achieve 80\% of the improvement brought by 120 seconds of real autotuning.
Our approach is not specific to filterbank correlation, or even to GPU autotuning: the approach of using a non-linear regression model on top of simple features applies to a variety of problem types, kernel types, and platforms.
A detailed model of the platform can provide useful features for cross-platform autotuning, but such a model is not necessary for same-platform autotuning.

\end{abstract}


\end{document}
