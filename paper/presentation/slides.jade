//TODO: get svg_include working
//TODO: latexpng

.slide
    svg_include(src='svg/title.svg')


.slide

    h1 Desiderata (user)

    ul
        li libs with conventional interfaces
        li platform independence

    h1 Desiderata (library provider)

    ul.incremental
        li help !?

.slide
    h1 Library Provider's Dilemma
    ul.incremental
        li platforms are different
        li different-sized inputs change everything
        li good performance across the board is HARD

.slide

    h1 Library Provider's Dilemma (part 2)

    h3 High-level:
    ul
        li blocking strategy
        li global and shared memory layout
        li synchronization vs. recomputation

    h3 Low-level:
    ul
        li loop unrolling
        li register spilling
        li registers vs. shared memory
        //li global / constant / texture memory


.slide

    h1 Library Provider's Options

    ul
        li Empirical Auto-Tuning

        li Model-driven Tuning


.slide

    h1 SoA 1: Empirical Auto-tuning

    ul.incremental
        li instrument your implementation
        li actually run the fn many ways, measure time
        li installation saves ptr to fastest config
        li Pro: very fast implementations
        li Con: what if the input array size changes?
        li Con: what about different hardware revisions / generations?

    //p In these cases you're on your own, empirical auto-tuning offers no help.


.slide
    h1 SoA 2: Model-Driven Tuning

    ul.incremental
        li instrument your implementation
        li write a hardware simulator
        li pick fastest config according to simulator
        li Pro: general, high-performance JIT approach
        li Con: who is going to provide simulators for all platforms?

.slide
    h1 Predictive Auto-tuning: Best of both worlds

    ul.incremental
        li instrument your implementation
        li instrument your problem
        li instrument your platform
        li measure time for many combinations
        li fit a timing model using machine learning

.slide
    svg_include(src='svg/predictive_auto_tuning.svg')

.slide

    //h1 filterbank correlation

    //p instance of a stencil computation

    //p cartoon and math

    //-
        latex2png:
            \begin{equation}
            \mathbf{z}[r,c,k] = \sum_{w=0}^{W-1} \sum_{h=0}^{H-1} \sum_{d=0}^{D-1}
            \mathbf{x}[r+h, c+h, d]~ \mathbf{f}[k, h, w, d].
            \label{eq:z}
            \end{equation}

    //p application domains that care

    //p show ranges of problem dimensions, relevance of different cases.

    svg_include(src='svg/fbcorr.svg')

    //R = C & \in \{ 256, 512, 1024, 2048, 4096 \} \\
    //H = W & \in \{ 3, 5, 7, 9, 11 \} \\
    //D &  \in \{1, 4, 8, 16, 32, 64, 128, 256 \} \\
    //F &  \in \{1, 4, 8, 16, 32, 64, 128, 256 \}

.slide

    h1 Filter bank correlation implementation (see file)

    //-
        latex2png: //TODO: implement this filter in jade
            \begin{algorithm}{$\Algo{thread\_fbcorr}\big(gX, cF, gZ \big)$}
            \Aitem shared $sX \setto$ all channels of region ($\beta$) of $gX$
            \Aitem $x, y \setto$ position of this thread in output image
            \Aitem \_\_syncthreads()
            \Aitem $v[0:N] \setto 0$, for $N=4\times n\_output\_4s$
            \Aitem for $d \setto 0$~\To~$D$,
            \Aitem ~ for $h \setto 0$~\To~$H / n\_filter\_r$,
            \Aitem ~ ~ for $w \setto 0$~\To~$W$,
            \Aitem ~ ~ ~ $u \setto sX[x+h, y+w, d]$
            \Aitem ~ ~ ~ for $n \setto 0$~\To~$n\_output\_4s - 1$,
            \Aitem ~ ~ ~ ~  $v[n] \setto v[n] + cF[n, h, w, d]$
            \Aitem for $n \setto 0$~\To~$n\_output\_4s - 1$,
            \Aitem ~  gZ[x][y][4n:4n+n] += v[4n:4n+n], (float4)
            \end{algorithm}



.slide
    // ALL THE OPTIONS SO MANY OPTIONS!

    ul
        li block height  (4 / 8 / 16 / 32 / 64 / 128)
        li block width  (4 / 8 / 16 /32 / 64 / 128)
        li n. filter rows  (1 / 2)
        li n. output 4s (1 / 2 / all)
        li spill (y / n)
        li 24bit imul (y / n)
        li pad shared (y / n)
        li use tex1d (y / n)
        li maxrreg (8 / 16 / 20 / 24 / 28 / 32 / inf)
        li fast math (y / n)

    //-
        latex2png: // TODO: add this to jade
            \begin{align*}
            \mathrm{block\_h}    & \in (4, 8, 16, 32, 64, 128) \\
            \mathrm{block\_w}    & \in (4, 8, 16, 32, 64, 128) \\
            \mathrm{n\_filter\_r} & \in (1, 2) \\
            \mathrm{n\_output\_4s} & \in (\mathrm{all}, 1, 2) \\
            \mathrm{spill}      & \in (False, True) \\
            \mathrm{imul\_fast}  & \in (False, True) \\
            \mathrm{pad\_shared} & \in (False, True) \\
            \mathrm{use\_tex1d}  & \in (False, True) \\
            \mathrm{maxrreg}    & \in (8, 16, 20, 24, 28, 32, \infty) \\
            \mathrm{fast\_math}  & \in (False, True)
            \end{align*}


.slide

    h1 Mapping timing to a regression problem

    ul
        li log-speedup over reference configuration
        li invalid configuration? log-speedup epsilon

    //-
        \begin{equation}
        y^{(i)}
        = \log\left(\frac{\mathrm{speed}(a, b, c)}{\mathrm{speed}(a, b^{(\mathrm{ref})}, c)} \right)
        = \log\left(\frac{t(a, b^{(\mathrm{ref})}, c)}{t(a, b, c)} \right)
        \label{eq:y}
        \end{equation}



.slide

    svg_include(src='svg/regression_tree.svg')


.slide

    svg_include(src='svg/boosted_regression_tree.svg')


.slide

    svg_include(src='svg/rforest_argmax.svg')


.slide

    //h1 Results: matching empirical auto-tuning -> GOOD

    svg_include(src='svg/fig_main_R1.svg')


.slide

    //h1 Results: avg GFLOPs when using the wrong model -> BAD

    svg_include(src='svg/fig_allstars_mixup_580_R1.svg')

.slide

    h1 Results: how sensitive is the approach to the score of invalid points?

    svg_include(src='svg/fig_ntrain_munctional0_580.svg')

.slide

    h1 Summary

    p scientists rely on high-level mathematical abstraction boundaries.

    p new hardware platforms are driving up the number of implementations needed to satisfy those abstractions with good performance.

    p we presented a standard, inexpensive approach to managing this increasing complexity: Predictive Auto-Tuning

.slide

    h1 Summary

    p predictict auto-tuning centers on the idea of using empirical data to
    fit a model of system performance for the purpose of optimizing a
    particular instrumented piece of code.

    p we showed how to apply this approach to filterbank correlation, fitting
    model with boosted regression trees but the approach certainly applies
    beyond that regime.

    p makes it easy to trade off JIT cost vs. install cost vs. running efficiency.


.slide

    h1 Future Work

    p platform features (device inspection, micro-benchmarks) generalize to future hardware
    p how far can you scale to larger search spaces
    p model-driven profiling, debugging, optimization

