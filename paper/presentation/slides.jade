//TODO: get svg_include working
//TODO: latexpng

.slide
    svg_include(src='svg/title.svg')


.slide

    h1 Desiderata (user)

    ul.incremental
        li libs with conventional interfaces (math)
        li platform independence
        li 100% efficiency for all inputs

    h1 Desiderata (library provider)

    ul.incremental
        li help !?

.slide
    h1 Library Provider's Dilemma
    ul.incremental
        li platforms are different
        li different-sized inputs change everything
        li good performance across the board is HARD

.slide

    h1 Library Provider's Dilemma (part 2)

    h3 High-level:
    ul
        li blocking strategy
        li global and shared memory layout
        li synchronization vs. recomputation

    h3 Low-level:
    ul
        li loop unrolling
        //li register spilling
        li registers / shared / texture / const


.slide

    h1 Library Provider's Options

    ul
        li Empirical Auto-Tuning

        li Model-driven Tuning


.slide

    h1 SoA 1: Empirical Auto-tuning

    ul.incremental
        li instrument your implementation
        li actually run the fn many ways, measure time
        li installation saves ptr to fastest config
        li Pro: very fast implementations
        li Con: very slow search
        li Con: brittle ...
        ul.incremental
            li to different input array size
            li to different platform

    //p In these cases you're on your own, empirical auto-tuning offers no help.


.slide

    //h1 Results: avg GFLOPs when using the wrong model -> BAD

    svg_include(src='svg/fig_allstars_mixup_580_R1.svg')
    build fade_in(#autotune)
    build fade_in(#mismatch)

.slide
    h1 SoA 2: Model-Driven Tuning

    ul.incremental
        li instrument your implementation
        li write a hardware simulator
        li pick fastest config according to simulator
        li Pro: general, high-performance JIT approach
        li Con: who is going to provide simulators for all platforms??

.slide
    h1 Predictive Auto-tuning: <br> Best of both worlds

    ul.incremental
        li instrument your implementation
        li instrument your problem
        li instrument your platform
        li measure time for many combinations
        li fit a timing model using machine learning

.slide
    svg_include(src='svg/predictive_auto_tuning.svg')
    build fade_in(#emp)
    build fade_in(#tx)

.slide

    //h1 filterbank correlation

    //p instance of a stencil computation

    //p cartoon and math

    //-
        latex2png:
            \begin{equation}
            \mathbf{z}[r,c,k] = \sum_{w=0}^{W-1} \sum_{h=0}^{H-1} \sum_{d=0}^{D-1}
            \mathbf{x}[r+h, c+h, d]~ \mathbf{f}[k, h, w, d].
            \label{eq:z}
            \end{equation}

    //p application domains that care

    //p show ranges of problem dimensions, relevance of different cases.

    svg_include(src='svg/fbcorr.svg')
    build fade_in(#bottomtext)

    //R = C & \in \{ 256, 512, 1024, 2048, 4096 \} \\
    //H = W & \in \{ 3, 5, 7, 9, 11 \} \\
    //D &  \in \{1, 4, 8, 16, 32, 64, 128, 256 \} \\
    //F &  \in \{1, 4, 8, 16, 32, 64, 128, 256 \}

.slide

    h1 Filter bank correlation implementation

    ul.incremental
        li implementation of (Pinto and Cox, 2011)
        li for kernel pseudocode, see paper
        li (show file)

    //-
        latex2png: //TODO: implement this filter in jade
            \begin{algorithm}{$\Algo{thread\_fbcorr}\big(gX, cF, gZ \big)$}
            \Aitem shared $sX \setto$ all channels of region ($\beta$) of $gX$
            \Aitem $x, y \setto$ position of this thread in output image
            \Aitem \_\_syncthreads()
            \Aitem $v[0:N] \setto 0$, for $N=4\times n\_output\_4s$
            \Aitem for $d \setto 0$~\To~$D$,
            \Aitem ~ for $h \setto 0$~\To~$H / n\_filter\_r$,
            \Aitem ~ ~ for $w \setto 0$~\To~$W$,
            \Aitem ~ ~ ~ $u \setto sX[x+h, y+w, d]$
            \Aitem ~ ~ ~ for $n \setto 0$~\To~$n\_output\_4s - 1$,
            \Aitem ~ ~ ~ ~  $v[n] \setto v[n] + cF[n, h, w, d]$
            \Aitem for $n \setto 0$~\To~$n\_output\_4s - 1$,
            \Aitem ~  gZ[x][y][4n:4n+n] += v[4n:4n+n], (float4)
            \end{algorithm}



.slide
    // ALL THE OPTIONS SO MANY OPTIONS!

    ul
        li block height  (4 / 8 / 16 / 32 / 64 / 128)
        li block width  (4 / 8 / 16 / 32 / 64 / 128)
        li n. filter rows  (1 / 2)
        li n. output 4s (1 / 2 / all)
        li spill (y / n)
        li 24bit imul (y / n)
        li pad shared (y / n)
        li use tex1d (y / n)
        li maxrreg (8 / 16 / 20 / 24 / 28 / 32 / inf)
        li fast math (y / n)

    //-
        latex2png: // TODO: add this to jade
            \begin{align*}
            \mathrm{block\_h}    & \in (4, 8, 16, 32, 64, 128) \\
            \mathrm{block\_w}    & \in (4, 8, 16, 32, 64, 128) \\
            \mathrm{n\_filter\_r} & \in (1, 2) \\
            \mathrm{n\_output\_4s} & \in (\mathrm{all}, 1, 2) \\
            \mathrm{spill}      & \in (False, True) \\
            \mathrm{imul\_fast}  & \in (False, True) \\
            \mathrm{pad\_shared} & \in (False, True) \\
            \mathrm{use\_tex1d}  & \in (False, True) \\
            \mathrm{maxrreg}    & \in (8, 16, 20, 24, 28, 32, \infty) \\
            \mathrm{fast\_math}  & \in (False, True)
            \end{align*}


//.slide
//    svg_include(src='svg/predictive_auto_tuning.svg')


.slide
    h1 Feature Space

    ul.incremental
        li Problem configuration
        ul
            li N.filters, height, width, rows, cols, etc.
        li Implementation choices
        ul
            li block height, 24bit mul, use_tex1d, etc.
        li Platform features
        ul
            li ... Future work

.slide
    h1 Fitting a timing model (part 1)

    ul.incremental
        li x = (problem, implementation, hardware)
        li y = time to completion
        li fitting minimizes average cost(y, T(x))
        li boosting is easy for (f(y) - f(T(x)))^2
        li what f will make fitting do what we want?

.slide
    h1 Fitting a timing model (part 2)
    ul.incremental
        li we have reference implementation
        li our choice: optimize speedup
        li f(y) = log(yref / y)
        li invalid configuration?
        ul
            li define y = Î¶ * yref

    //-
        \begin{equation}
        y^{(i)}
        = \log\left(\frac{\mathrm{speed}(a, b, c)}{\mathrm{speed}(a, b^{(\mathrm{ref})}, c)} \right)
        = \log\left(\frac{t(a, b^{(\mathrm{ref})}, c)}{t(a, b, c)} \right)
        \label{eq:y}
        \end{equation}


.slide

    svg_include(src='svg/regression_tree.svg')

    build fade_in(#textbottom)


.slide

    svg_include(src='svg/boosted_regression_tree.svg')
    build fade_out(#loop_cover)
    build fade_in(#Tx)
    build fade_in(#bottomtext)


.slide

    svg_include(src='svg/rforest_argmax.svg')
    build fade_in(#algo)
    build fade_in(#search0)
    build fade_in(#search1)
    build fade_in(#search2)
    build fade_in(#search3)
    build fade_in(#bottomtext)



.slide
    svg_include(src='svg/fig_main_R1.svg')

    build fade_out(#overlay)


.slide

    svg_include(src='svg/fig_ntrain_munctional0_580.svg')


.slide

    h1 Summary

    ul.incremental
        li Scientists rely on mathematical abstraction boundaries.
        li New hardware demands multiplicity of implementations.
        li Predictive Auto-Tuning is standardized, inexpensive approach to providing JIT selection of fast problem-specific implementations.

.slide

    h1 Future Work

    ul
        li generalizing across hardware
        ul
            li device inspection
            li micro-benchmarking
        li faster forest optimization
    //p how far can you scale to larger search spaces
    //p model-driven profiling, debugging, optimization

.slide

    h1 Thank You
